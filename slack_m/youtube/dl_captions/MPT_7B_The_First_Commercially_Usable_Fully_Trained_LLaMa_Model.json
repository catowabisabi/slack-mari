{
  "title": "MPT-7B - The First Commercially Usable Fully Trained LLaMa Model",
  "url": "https://www.youtube.com/watch?v=NY0bLFqkBL0",
  "zh_summary": "繁體中文詳細摘要:\nMosaic公司最近推出了一款全開源的商業可用的MPT 7B模型，該模型經過一兆個標記的訓練，是Lama的關鍵，並且可以商業使用，包括基礎模型、導師模型、聊天模型和故事作家模型，導師模型是在Databricks Dolly 15K數據集上微調的，聊天模型使用了一些蒸餾數據集，因此不能用於商業使用，故事作家模型是在65000個標記的上下文中微調的，Mosaic還在LLM Foundry中發布了代碼，包括訓練、微調、評估，甚至推理和服務的代碼，與原始Lama模型比較的基準測試表明，在使用此框架時，一些數字和統計數據不能得到支持，訓練是在440個GPU上完成的，故事作家模型以《了不起的盖茨比》這本小說為測試，能夠在小說末尾寫出一個結局，訓練數據是由Red Pajama數據集、代碼、Arxiv和Stack Exchange組成的，訓練的計算是用A100 40GB和80GB卡完成的，微調只需要1000萬個標記，而且不到2.5小時就能花費37美元完成，而聊天模型需要近10倍的標記，而且花費超過4000美元完成。\n\nHuggingsFace最近發布了一款新的人工智能模型Instruct，它能夠理解自然語言處理，可用於摘要、文本轉換為JSON等各種任務，該模型在兩個半小時的數據集上進行訓練，需要80 GB的訓練，該模型還具有聊天界面和故事作家模型，該模型能夠回答像“alpacas、vicunas和llamas之間有什麼區別？”這樣的問題，並且能夠產生俳句，該模型還能從TechCrunch上摘要文章，該模型能夠產生摘要的槍點，但是對於推理任務和數學任",
  "en_summary": "Traditional Chinese Detailed Abstract:\nMOSAIC recently launched a fully open -source business -available MPT 7B model. This model has been marked with one mega.The instructor model is fine -tuned on the DataBricks Dolly 15K data set. The chat model uses some distilled dataset, so it cannot be used for commercial use. The story writer model is fine -tuned in the 65,000 label context. Mosaic is still in LLM FoundryCode, including training, fine -tuning, evaluation, and even the code of reasoning and service, the benchmark test compared with the original LAMA model shows that when using this framework, some numbers and statistical data cannot be supported. The training is on 440 GPUsCompleted, the story writer model is tested with the novel \"The Great Gatsby\", which can write an ending at the end of the novel. The training data is composed of the Red Pajama data set, code, ARXIV, and Stack Exchange. Training calculationsIt is completed with A100 40GB and 80GB cards. Finction only costs 10 million marks, and it can be completed in less than 2.5 hours. The chat model takes nearly 10 times the mark, and it costs more than 4,000 US dollars to complete.\n\nHuggingsface recently released a new artificial intelligence model Instruct, which can understand natural language processing. It can be used for abstracts and text conversion to JSON.Training, this model also has a chat interface and story writer model. This model can answer questions like \"Alpacas, Vicunas, and LLAMAS?\" And can produce a sentence.This model can produce abstract gun points, but for reasoning tasks and mathematics appointments",
  "cn_summary": "繁体中文详细摘要:\nMosaic公司最近推出了一款全开源的商业可用的MPT 7B模型，该模型经过一兆个标记的训练，是Lama的关键，并且可以商业使用，包括基础模型、导师模型、聊天模型和故事作家模型，导师模型是在Databricks Dolly 15K数据集上微调的，聊天模型使用了一些蒸馏数据集，因此不能用于商业使用，故事作家模型是在65000个标记的上下文中微调的，Mosaic还在LLM Foundry中发布了代码，包括训练、微调、评估，甚至推理和服务的代码，与原始Lama模型比较的基准测试表明，在使用此框架时，一些数字和统计数据不能得到支持，训练是在440个GPU上完成的，故事作家模型以《了不起的盖茨比》这本小说为测试，能够在小说末尾写出一个结局，训练数据是由Red Pajama数据集、代码、Arxiv和Stack Exchange组成的，训练的计算是用A100 40GB和80GB卡完成的，微调只需要1000万个标记，而且不到2.5小时就能花费37美元完成，而聊天模型需要近10倍的标记，而且花费超过4000美元完成。\n\nHuggingsFace最近发布了一款新的人工智能模型Instruct，它能够理解自然语言处理，可用于摘要、文本转换为JSON等各种任务，该模型在两个半小时的数据集上进行训练，需要80 GB的训练，该模型还具有聊天界面和故事作家模型，该模型能够回答像“alpacas、vicunas和llamas之间有什么区别？”这样的问题，并且能够产生俳句，该模型还能从TechCrunch上摘要文章，该模型能够产生摘要的枪点，但是对于推理任务和数学任",
  "zh_paraphrase": " 不好意思，我不知道。\n 不好意思，我不知道。\n 不知道。\n",
  "dialogue": "In this video, I'm going to be\nlooking at the MPT 7B model., so this is a new model from\na company called mosaic., and they specialize in training models., So they're actually, the people who\ntrained up the recent replica code models., and they've gone and trained up., They own a fully opensource,\ncommercially usable., Lama 7 billion style model., So if we have a look at this, For a start., This is pretty amazing that they did\nit, a startup being able to do this., also pretty amazing that they managed\nto do it in just nine and a half days., So the book post is very good at going\nthrough, a bunch of different stuff., they've trained up to a trillion\ntokens, which is the key, thing that we want for Lama., And, they've also been, just made\nit all commercially available., So there., the base model is the key\nmodel for that's the equivalent, of the 7 billion Lama model., but they've also then fine\ntune some other models as well., So they've got an instructor model., which they've fine tuned., on the Dolly, dataset., We'll look at that in a second., They've got a chat model, and\nthey've got a story writer model., So unfortunately the chat model users,\nsome of the distilled data sets so, that one's not for commercial use,\nbut the other ones are for commercial, use and you can also then fine tune\nthem yourself on your own data., for commercial use., So this is in many ways what we've been\nwaiting for, An open source LLaMa model., that's high quality., stable LM came out with\nsomething that was, I think, trained for 800 billion tokens., But it turned out when you looked\ninto it that they'd actually made, some mistakes in the training., for quite a few of the tokens earlier on., And what we've seen, models\nfrom, red pajama And open Lama., these have been just snapshots of\nmodels that are currently training., This is the first one that we've\nactually got where it's the full, model it's finished training., it's being benchmarked to show that\nit's basically on par with Lama., and they've given us some fine\ntuning stuff for it as well., they make a big deal rightly so of,\ntalking about this commercial use thing,, totally kudos to them for doing that., That's really good that they've done that., Some other things that they've\ndone though, which are really cool., is, they've trained with, alibi here., So this is, allows us to\nactually extend the context., Of these models., So if you wanted to fine tune up., a longer model, you can actually do that., And that's what they've done\nwith this story writer model., So I they've taken, the base model\nand then they've done, a fine tuning, with context length of 65,000 tokens., so just to put that in, in\ncontext, the original, Lama model., was 2048 tokens., the stable LM one, I think\nthey're training at 4,096 tokens., ChatGPT is 4,096., GPT-4, depending on which\nversion you've got access to, is, 8,000 to 30, 2000 tokens., And then this has basically doubled that., So I, it's pretty impressive feat\nthat they've managed to do this., And I think this really now opens it\nup to the community to start playing, around with these things and getting,\nversions of these kind of models that, maybe aren't going to be 8,000 tokens,\n16,000 tokens, perhaps even 32,000, tokens for a lot of different tasks., for this kind of thing., All right., So they've also trained\nup a nice, instructor., We've got the base model,\nwhich they talk about here., This, this base model includes\nflash attention, which., and, alibi, which definitely the,\nthis I noticed in playing with this,, that the inference speeds are faster., Then some of the other 7 billion models., And I think this is because of the\nthings that they've used there., I have done the storyteller\nmodel, which I've talked about., They've done., a 7 billion instruct model., and this is short form., instruction following., So this is on a dataset that's\nthey talk about as being derived, from the Databricks Dolly., 15 K dataset., I remember, that's the one\nwhere Databricks got their, employees to write up a dataset., And, anthropics helpful and harmless., dataset now., I think., This works out to be roughly 59,000., examples for training., in here., the good thing with this model is\nit doesn't seem to have the whole as, an AI language model stuff, because\na lot of it was written by humans., So you can go through it., And when you play with it,\nif you ask it questions., At least in my testing, you\ndidn't seem to get the whole, oh., As an AI language model., I can't ex., Y Z, whatever kind of thing., This is definitely though, not as big\na data set as the shared GPT as the., Vicuna, models and the koala\nmodels are using for this., So you definitely notice that when\nyou use it, But it's certainly, fantastic that they've released this., you can play around with this., And try it out., we'll look at in second, the training\ntime and costs for these as well., I the, and then the last one that\nthey release is a chat model., So this is, basically fine tuning on., the sharedGPT Vicuna and a number\nof other, uh, datasets, for this., And, this one is not for commercial\nuse because of the datasets, that they're using in there., but it does show to show, okay., How this would compare to\nsome of the other ones., So it's certainly with, you., Playing around with it\nand having a look at., at the model for that., So another thing that they've done,\nwhich is really good is that they've also, released a whole bunch of code in this., what they calling their LLM Foundry., So if we jump in and look at the LLM\nFoundry, we can see that, this has, got code for training for fine tuning., and for evaluating and then\neven for inference and serving,, for these models as well., so that's something that's\nvery cool to look at., And it's been pointed out that,\nThey're benchmarking when they're, comparing to the original Lama\nmodel, shows that definitely., some of the figures and stats\nfor the original alarm and model., Don't hold up when tested\nwith this framework., so that raises a whole bunch\nof other issues that I'm not, going to go into here, but it's., It's interesting., that perhaps they were using some\nsort of special prompts or something, that they haven't really disclosed., in the paper itself., So in here., they talked about, they trained\nwith zero human intervention., on 440 GPU's., and they actually, we'll\nlook at the training., in a second., it's pretty amazing that this\nwhole thing is just automated now., So if you're a company looking to do this\nkind of thing, you could use this library., You could use them as a service provider\nto actually train up your model., So MPT, what does it stand for?, It stands for mosaic\npre-trained transformers., And this is, A set of these, that\nthere, that they've released and we, may even see some more, I'm not sure if\nwe're going to see a 13 billion here., My guess is that may depend on,\nfinancial factors and stuff like that., And if they're going to try and\nsell that to companies, perhaps., So when we look at the stats\nhere, we can see that they've, benchmarked the base model., so this is their model at the top., and this is the., Metta AIS Lama model., And you can see that there,\nthere are some differences., for these are models on some of\nthem, the MPT winds and some of, them, the original Lama wins., But on the whole, they're\npretty close to each other., So that's something to just, notice\nthat this is definitely a model that's, on par with a LLaMa 7 billion model., So I mentioned before, one of the\nreally interesting things is this, Story Writer model, where they have a\ncontext window of 65,000 plus tokens., and they actually even talk about\nthat, when they did inference,, they even did it, beyond that., so one of the things\nthat they did for this., Was, they took the entire,\nnovel of the great Gatsby., And it turns out that is works\nout to be 67,000, 8 73 tokens., And then got it to write., And you epilogue at the end of it for it., and it looks, interesting,, Because this is one of the things\nthat people have held as the holy, grail of, being able to put a whole., long form document, like a novel or\nsomething in and have the model be able to, pay attention to various parts of the, of\nthe actual document and then use that to,, come up with, things in its generation., And it certainly looks like that's., what's going on here., so this is definitely an interesting., area that we're going to see a lot\nmore of going forward in the future., All right., So just a quick look at their\ntraining data for the pre-training., it's made up in a very\nsimilar way to Lama., They've used some of the red\npajama dataset that was released., recently., They've also got code in there., They've also got Arxiv in there., They've got stack exchange in there., so this is a very good general,\nbroad, pre-training dataset., For the compute for training this, they\nused, A100s, and they use both the 40Gb, ones and the 80Gb ones by the looks of it., they also show for the actual., Fine tuning of these models,\nwhat it took as well., So we can see here that the original\ntraining was done with 440, a 100., 40 gigabyte cards., And we can see the, the\nbatch size for this., We can see the training context link\nfor this was the same as, as Lama there., but then we can also see the\nfine tuning so we can see, okay., The number of tokens for doing\nthe fine tuning for instruct., It's actually quite small, right?, It's only a under 10 million tokens., and we can see that only costs $37 to\ndo, In sort of two and a half hours, at less than two and a half hours., this, that, that's pretty interesting., the chat one took quite a bit more\nbecause it was quite a bit more., almost 10 X., the amount of tokens., And we can see that., also the story writer, one was quite\nexpensive to do in the, this required the, bigger, a one hundreds with 80 gigabytes., And, took, for over $4,000\nworth of training here., I, but it's interesting to think that., These models once they're fine\ntuned on much longer context., we going to then be able to find,\ntune them for other things as well., that a really long context in\na very easy and quicker way., this is something that, the community\nis going to need to test going out., All right., Let's have a look at\nwhat they've released., So they've got the dataset., for this, the fine tuning for\ninstruct, which you can go in and, get access to straight away and use., They've also released a HuggingFace\nspaces for the instruct., So you can just come in here., and try out, that., very simply., They've also released a\nchat interface for this., hung face spaces where you can\ncome along and chat to the model., And try it out., quite easily and stuff like that as well., And they've gone on to release the\ndataset for the chat as well here, that we can see, Of what they've\nactually gone through and done there., So obviously on top of the\ndatasets and the, spaces, they've, also released these models., So we can see that the., story writer model is here., The chat model is here., The base model is here and\nthe instruct model is here., So let's jump into some code., to just go through this., So the interesting thing with what\nthey've done compared to a lot of the, other people training these models,\nis they've gone with quite a lot of,, custom code for improving inference\ntime for, adding things to this., on one of the, one of the things\nthat they've done also is set, up their own sort of pipeline., So they've got this instruction., in text pipeline in here., you can use this if you want to,\njust make it easier for doing., generation with this., you can come in here and set the various\ntemperatures and stuff like that., I played with a little bit of these., I ended up going., to the settings for what they\nhave on the hugging face spaces., which seems to use a very low temperature., and I found sure enough that if I\nmade this too high, I, it seems., That it wants a lower temperature\nthan some of the other models., have in the past., anyway, once we've got that,\nwe can just bring it in., we need to have this trust\nremote code equals true., for it to be able to\nbring in its own code., for running some of the models\nand for the inference, et cetera., Once I set this up so that they\ndo have their own prompt for this,, which I've incorporated in here., that we can use, and then we\ncan just use it like a normal, instruct model so we can ask it., What are the differences between\nalpacas vicunas llamas you see,, it's giving a decent response back., If you asked at what is the\ncapital of London, all these., standard things that we've\ndone for the other models., So I'm not going to go\nthrough reading them out., I do like that., It, for this kind of question where you\nsay as an AI, do you like the Simpsons?, Since I don't have any feelings, but\nI can tell you what's on Wikipedia., and this is a nicer way I think of\ndoing it rather than, what we've gotten, a lot with the distilled stuff from\nthe ChatGPT, where it basically is, just constantly saying, oh, as an AI\nlanguage model, I can't, this and that., so anyway, I played around with,\nDoing this I find on the instructing., Sometimes it will give\na very short, responses., I'm not sure if that's just\nbecause of the way that the Dolly., once our, or if they've, if they've\ndone any filtering, I don't think, they've done a, a massive amount of\nfiltering or anything like that here., but I can do things like, tell\nme about, Homer on the TV show., Simpsons., And I put in depth., to get it to go a little bit further., It's definitely not in depth for here., you know what, at least\nwhat I would think about., it doesn't do well with the reasoning\ntasks, trying it with a few of, the different reasoning tasks., It didn't do well with\nthe math kind of stuff., it does get some of them, right?, Like the haiku one., This one, you often generate a few times., and sure enough, it will answer yes., Each time, but it will sometimes be\nvery succinct and just saying, yes,, it's possible or, that kind of thing., Whereas, occasionally it will actually\ngive you a Haiku too, which is quite nice., Also., I turned it out with their examples\nof converting something to JSON., Did a really nice job of that., just for basic chat., It also seems to do okay., for this, write a short plan,\nfor some of these things., Another thing., I tried it out for,\nwith summarization too., So here I just copied and pasted it., An article from tech crunch shop., A chunk of an article from tech\ncrunch and it does a pretty good, job at summarizing that article., to get the sort of major facts out of it., And then if I ask it to do it\nin bullet points, it's able, to put them in bullet points., Now I did try, getting a lot more\nbullet points with a shorter., facts and stuff like that., I didn't have a lot of success with\nthat, but I'm sure that's going to, be just how you play with the prompt., to get that kind of result., out with this kind of thing., anyway on the whole,\nit's a pretty nice model., will put this one up for you to play with,, I could have a quick\ngo at the story writer., one and, it is pretty cool., doing it but i've i find that i'm not\ngetting great results beyond sort of, 8,000 tokens so i'm going to play around\nwith this a little bit more and i might, actually do a whole video about this one\nin the future and then the chat one it, is Similar this is probably the closest\none to like the Vicuna models the koala, models that kind of thing Honestly i do\nat least in my small testing was i didn't, find it to be as good as those ones anyway\nhave a play with the with the code for the, instruct one you can try it out and see\nhow it goes I'm sure we're going to see, a lot of fine tunings of the mpt seven B., base model i know i'm certainly going\nto be doing that i'll probably make, some videos at some point of showing\nsome different fine tunings and doing, some things that you can do with it\ni've been waiting for a LLaMa model that, we can use commercially this certainly\nlooks like it's the first one My guess, there's over the next month we're going\nto see a few of them so we're not going, to know which one is going to be the\nbest one until they're all out probably, i think we've got red pajama coming\nthrough We've got stability AIs models, coming soon as well and the open open\nlama project as well and a lot of those, have released models already but they're\nactually just partially trained models, so it is going to be really interesting\nto get the fully trained versions of, those and see actually how good they are., Anyway as always if you have any\nquestions please put them in the, comments if you like the video please\nclick and subscribe i will talk to, you in the next video Bye for now"
}