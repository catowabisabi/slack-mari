{
  "title": "MLC LLM: Enabling LLMs To Be Deployed Across Multiple Devices",
  "summary": "\n\"重點1\":\"MLC LM是一個新的尖端技術，可以提供一個通用的解決方案，用於在廣泛的硬件後端以及原生應用程序（如Nvidia和不同的安裝）上部署任何語言模型。\"\n\"技術1\": \"MLC LM支持多種硬件和軟件平台，包括Apple設備上的metal GPU，Windows上的Vulcan，Linux上的Cuda，以及Web GPU。\"\n\"重點2\":\"MLC LM旨在使其較容易集成到現有的工作流程中，並支持流行的機器學習框架，如TensorFlow或",
  "dialogue": "foreign hey what is up guys welcome back, to another YouTube video at the world of, AI in today's video I got something, amazing and that is mlclm and this is a, new Cutting Edge technology that offers, a universal solution for deploying any, language model on a wide range of, Hardware back-ends as well as native, applications such as Nvidia and, different like installations on, different Hardware back-ends with, different gpus now basically its core, feature is to provide a productive, framework that allows developers as well, as users to actually optimize model, performances for their specific use, cases now this is something that we're, going to be exploring in today's video, by going a little bit more in depth as, to what they're trying to accomplish as, well as get a better understanding of, some of the actual cons and pros of this, actual application as well as what this, project is trying to do we're also going, to take a look at how you can actually, install it onto your actual Hardware, there's different ways in which you can, do so so I'll definitely showcase that, and with that thought guys I just want, to say thank you guys so much from the, bottom of my heart it really means so, much to me I know I haven't uploaded for, the last couple of days because I was, out of town doing a lot of work so I, wasn't able to actually prioritize and, upload I'm really sorry for that guys, but I'm back on my regular schedule and, you should expect two to three videos a, day now I just want to say thank you so, much for all the love and support you, guys have been giving me guys because it, really means so much to me I had not, expected to see a channel grow this fast, within like four weeks probably now I, had no expectations I was just making, these videos for the fun of it guys, because if you I'm just like a new grad, doing stuff like this and it just means, a lot to me guys because sharing as well, as like creating content that helps, people really means a lot to me because, like in the joy of others lies our own, and helping other people definitely, means a lot to me and what you guys have, done for me has been remarkable and I, I'm gonna continue to work my hardest to, improve my speech my content and the, value that I provide to you guys because, it really means a lot to me for all the, support and all the care that you guys, actually give me and thank you guys so, much for the people who have been, supporting me on this you guys have been, actually donate and I did not expect, anyone to do so it really means a lot, because I'm not using this for my, personal gain guys because I'm putting, this all back into this channel I, actually used some of this to purchase, an API key because I didn't have the, funds to actually do that before and now, I actually did it so this way I can like, showcase different things by doing, actual demos using an API key so it, really means so much to me guys that you, guys have been supporting me and with, that thought guys let me get right back, into the video sorry I was rambling I, just wanted to show my appreciation to, all you guys now if you guys haven't, seen any of my previous videos it would, mean the world to me guys if you guys, can do so if you guys aren't subscribed, please do so and like this video and, with that thought let's get right into, the video, so what is mlclm now I talked about a, little bit but let me give you a little, bit more details on it now basically, based on the details available from, their actual repo we're able to, understand that mlc LM is a machine, learning interface engine that enables, the deployment of natural language, processing so this way it's easier for, you to access different models on, different hardvers and this enables the, deployment of different NLP models on a, range of platforms and this is something, that's quite remarkable guys because you, don't see chat gbt on your phone or, different applications it's something, that you can only access on the desktop, at the current moment obviously you can, do it now through your mobile device but, you're not able to access different, models maybe uncensored ones on your, phone or different Hardwares and, basically what this project is trying to, do is it's trying to include these other, Hardwares that so that you're able to, support it on different Hardwares and it, supports a variety of programming, languages guys because it includes, python C plus plus us Swift and, different like gpus and softwares that, is that makes this actual whole project, more applicable for different people, with different use cases and it's going, to make it so much accessible to a wide, range of developers as well and I feel, like this is going to be one of the most, remarkable things that we're going to be, able to see in the development in the, coming days as well as the coming weeks, now one of the key features of this, Amazing Project is that its ability to, optimize models for specific use cases, now what we're going to be showcasing is, like a flow chart of how it actually, works but basically it enables, developers to fine-tune Performance and, accuracy for specific applications, you're able to optimize for different, gpus as well as different hardware, back-ends and the Technology support, models basically helps you reduce the, size of the model and it enables faster, interface so you're going to be able to, do this so that you're able to remove, unnecessary weights and connections to, improve the actual perform of what, you're trying to run on different, Hardwares, now this LM supports a range of Hardware, back-ends including metal gpus on Apple, devices you're able to use Vulcan on, Windows Linux Cuda, I believe is NVIDIA gpus as well and it, also includes the support for web GPU, this is another project that I actually, made a video on so I highly recommend, that you check it out and it basically, enables an interface directly on your, browser which you can run on Chrome now, in addition to supporting a range of, hardware and software platforms this LM, is also designed to make it so much, easier to integrate into existing, workflows so it includes command line, interface for like quick development as, well as supports popular machine, learning Frameworks such as maybe like, tensorflow or Pi source so that you can, utilize it as a developer for different, use cases and you're going to be able to, make a better impact using this actual, llm, so let's take an actual look as to how, it works in terms of its flow chart now, based off of this actual demo we're able, to get a better understanding of, breaking down the several key steps so, there's actually four steps as to how, this operates the first one is the input, encoding and this is where the first, step is of this actual flow chart and it, is the input encoding where the text, input is converted into a numerical, representation and basically it is then, processed by a large language model for, example in this case they have baikuna, so and it processes it by the actual, language model and this is typically, done using methods such as tokenization, where the actual text is split into, individual tokens words phrases or Etc, and it's then mapped into corresponding, normical values basically we've seen, this with many different LMS where it's, the actual information is split into, different chunks so that the encoders, can utilize as well as pick out certain, chunks that are actually related to The, Prompt and this is what the actual, application is doing and the second step, is actually the embedding where in the, embedding step that an actual numerical, representations of the input is then, transported into the actual High, dimensional Vector that captures the, somatics meaning of the actual input so, then gets a better understanding of the, numerical chunks and this is done by a, pre-trained embedding model which they, have installed installed sorry into the, actual flowchart I mean the application, and this Maps the numerical values to a, high dimensional Vector space and then, what follows after is the third step, where the model enter, like basic interference of once the, actual input has been encoded and, embedded what happens is that the, language model then performs an, inference to generate the output and, this typically involves like the, processing of the input through several, layers of neutral as well as neural, networks which learns to identify, patterns and relationships of what is, being sent from the actual numerical, chunks, and obviously from this you're able to, get the output decoding where this is, the final step and this is where the, output of the language model is, converted back into a human relatable or, readable text and this is typically done, by the methods such as beam search where, the model basically generates multiple, different output sequences and it, selects the most likely sequence based, off the set criterias of your initial, prompt that you've given and this is how, it works guys and obviously I'm gonna, this is just a little background of how, the actual system works of the flowchart, of the actual application but what we're, going to be doing is taking a look at, how it operates in terms of the, potential use cases as well as how it, can functionally operate within, different Hardwares and gpus, so what this project has actually done, is they're trying to obviously, incorporate into different Hardwares and, gpus but what they're trying to do is, the potential have the potential use, cases of this element on consumer, devices to be brought out with different, use cases such as personalization using, it on offline support having it, decentralized which is absolutely, amazing specialization and app embedding, so that it could be used with, third-party vendors to incorporate in, certain use cases and obviously with the, first case which is the concept of, personalization and this is where the, large language models are able to, actually perform many different tasks, but may not know our personal, preferences such as our favorite music, or like our writing style now in the, future what they're trying to do is so, that there may be a demand of a, personalized AI companion that is, tailored to our individual needs and it, can be used to enhance our daily, productivity now these models could work, alongside with what our actual needs are, so it uses our personal data to, basically give the best output and it, creates this personal AI companion where, we would need to like basically feed, personal data to the model and it would, ideally run them off of our own devices, so we can get the best output using our, personal preferences the second concept, is the application integration which we, talked about and this is where large, language models can be adapted to, specific use cases with different, third-party vendors and this way we're, able to get the best experience of the, application so in this case it is a game, and it could be incorporated with the, story specialized LM dialog model and, this way we're able to generate, different ways for the game to use the, large language model to give the best, output, now the third concept is an offline, support and a client server hybrid use, case and this way you're able to upload, uh maybe an offline model of the large, language models have all the weights put, onto a different cloud and it utilizes, the cloud to generate your prompts, offline without any use of basic like, internet use and this way you're able to, generate anything that you want without, any sort of like internet connection or, network and the final step is, decentralization and this is where the, power or the computer power of an, individual consumer device may be less, powerful than the of an actual data, center but when it's actually connected, together it can perform powerful tasks, by utilizing different neural networks, of different Hardwares and this way, you're going to have the potential of, this concept where it can give you the, best connection throughout different, outputs as well as different inputs to, get you the best use case of this, application on different Hardwares and, this is something that they've actually, emphasized on guys in the future cases, of what this LM can accomplish now this, is something that's quite remarkable, because it's going to be, something that they're going to develop, over time and obviously there's going to, be certain limitations that they're, going to work towards fixing but this is, just the start guys and we're gonna see, a lot more as to what they're trying to, do now in terms of what their actual, goals are I wanted to emphasize what, this is going to be what they're trying, to accomplish and how they're going to, be able to do so now obviously we know, that, the primary goal of this project is to, make it possible for AI models to be, developed and optimized and basically, deployed for interfaces on a wide range, of devices and this is something that's, amazing because AI has been basically, popping off in these recent like years, and what people want to do is actually, get the use cases of it and what this, will obviously do is make people's, day-to-day lives way more easier as, you're going to be able to access with, like different like models using this, actual application because it'll be way, more accessible on different fronts and, I believe that with the addition of this, amazing software we're going to be able, to, like enhance the basic day-to-day needs, as well as ones of different people's, tasks on a day-to-day front and you're, going to be able to like basically make, life so much easier on a wide skate, scale range so this is going to be quite, groundbreaking in my opinion because, it's gonna make it more accessible like, large language models in generally more, accessible to a wide range of people, across the whole world and I definitely, feel like this is going to be a quite, remarkable because it's going to do so, much good for a lot of different people, and I hope you guys can get a chance to, check this out now in terms of, installing it you want to go on to the, mlc website and I'll leave all the links, down in the description below and you, can install this actually on a local GPU, as well as your phone and laptop, and it runs fairly easy on these, processors so definitely recommend that, you check it out I'm not going to go, through in each like a detailed like, explanation as to how you can do it onto, your like processor or model because, it's quite extensive and there's if you, want me to make a video on it I can, definitely do so but if you want to, check it on the iPhone there's actually, a demo as to how you can do it you can, install it on Windows as well as on your, local GPU, and that's basically it guys you can, also ask access it using web llm and, this is basically a companion project, that helps you operate it off of chrome, so I definitely recommend that you check, this out all the links will be in the, description below and definitely read, through the disclaimer before actually, working towards using this because it's, basically meant for research purposes at, the moment and that's basically it for, today's video guys I hope you had a fun, time as well as got a lot of value out, of today's video thank you so much for, watching guys I really really appreciate, it if you guys haven't seen any of my, previous videos I highly recommend that, you do so there's a lot of value that, you will definitely benefit from so with, that thought guys thank you so much for, watching and I'll see you guys next time, peace out fellas have an amazing day, have a great smile and I'll catch you, soon peace out fuzz",
  "zh text": null
}