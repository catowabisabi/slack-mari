{
  "title": "The LangChain Cookbook Part 2 - Beginner Guide To 9 Use Cases",
  "url": "https://www.youtube.com/watch?v=vGP4pQdCocw",
  "zh_summary": "",
  "en_summary": "",
  "cn_summary": "",
  "zh_dialogue": "",
  "dialogue": "welcome to the langchang cookbook part 2, where we're going to cover the nine, major use cases of Lane chain in part, one we went over the nuts and bolts of, Lane chain but not necessarily the, applications my goal for this video is, to inspire you to build no one has, expertise in your domain like you do, really I have no idea what you do but I, do know that U plus AI can come up with, some really cool applications that can, create value in the world in addition to, this video I've also started a gallery, of Lang chain projects for you to get, more inspiration from check out the, homepage of this repository to see how, other people are creating tools of, language if you'd like to get your, project featured check out the, contribution guide and throw up a PR if, you'd like to follow along with projects, on building or AI trends that I'm, excited about you can check out my, journey on Twitter last thing that I'll, say is this video assumes that you've, seen part one of the series I'll be, light on details about how these, Frameworks work and more focused on the, application side all right let's jump, into some code my first goal this, cookbook is like I said inspire you to, build my second goal is to provide you a, working understanding of the main use, cases of light chain in super easy to, follow examples with code Snippets now, if you want to know the nuts and bolts, of LinkedIn head over to the, fundamentals video that's linked in here, this code can be accessed with a link in, the description first video we talked a, lot about what is link chain and I won't, go through all these details again but, the tldr is LinkedIn is a framework that, helps you build applications powered by, language models so if you start working, with your own language models you'll, quickly see that there's a lot of really, complicated parts and Lane chain is a, framework to abstract away those, complicated parts and get you building, faster these are the nine main use cases, that we're going to go over now I won't, repeat them right here because we're, going to go into them with some detail, later on in the notebook but if you'd, like to jump to a specific section in, the video I put in timestamps and so you, can check it out for yourself let's dive, into the first one summarization all, right first thing we're going to do is, insert our openai API key you can BYO, language model if you want so you can, bring your own but I use open AI just, because it's nice and easy and well, supported okay next thing I'm going to, do is I'm going to make my display just, a little bit wider this is optional you, don't have to do this if you don't want, to all right first use case that we're, going to look at is summarization so, this is a very common topic within Lang, chain and the gist of this is to extract, the important details or meaning from a, longer body of text now in each section, right here I'm going to put a link to a, deep dive uh coming soon if I haven't, found the right resource or I haven't, made it yet I'm going to link you to, some examples and then I'm also going to, link you or tell you about some use, cases that I've seen other people use, this section for so in the case of, summarization I've seen people summarize, articles transcripts chat history blah, blah a whole bunch of really cool stuff, here let's see how to do summaries of, short text well in this case I'm going, to import open AI I'm going to import, prompt templates and then I'm going to, create my language model now the first, thing I'm going to do is get my template, going now in this case I wanted to show, you an extremely easy example about how, a summarization can be done because then, we'll build on top of that all I'm doing, is I'm making a prompt that has some, instructions please summarize the, following piece of text respond the, manner that a five-year-old would, understand and then I put a placeholder, right here for the text and using a, prompt template what we can do let me, run that so, and then what we can do is uh go grab, some confusing text or some long text or, whatever it is you want to summarize I, went and found this article from a, science Magazine online and then what, we're going to do is we're going to, store that in a confusing text variable, and then I'm going to create my actual, prompt by calling prompt dot format and, inserting my confusing text which is, going to get insert in the token that we, looked at above here and if we wanted to, look at the final prompt about what this, would look like we still have our, instructions but now the confusing text, has been placed in there and let's go, ahead and run this to get our output and, see we can get here all right we have, our output and with a nice couple, sentences we get our summary we could, reduce this even further or make it, longer you just need to instruct your, language model to use something, different when you give it the, instructions up at the top now this, method works fine but it's pretty manual, and if we wanted to do longer text well, we're going to need a different method, for that so I'm going to go ahead and, import my link chain packages again but, the new one this time is I'm going to, have the load summarize chain and this, is going to be an easy abstraction for, course that is going to be able to, handle longer summaries so now I'm going, to load up a longer document and just to, show you this is a Paul Graham essay and, I'm printing the printing the first 285, characters and you can see here it's, just kind of a standard essay that he, did now the important part is we want to, see how many tokens are there in here so, there's uh, 3970 tokens which this would fit inside, of the token limit for most of your, models that you're using but just work, with me here and pretend like it doesn't, so the next thing I'm going to do is I'm, actually going to split this document up, in different chunks because I want to, get a summary of the mini chunks and, then get a summary of the summary let's, go ahead and run this so now we have, four documents instead of one piece of, text and so this one big file got split, up into four pieces then what I'm going, to do is I'm going to create a chain and, this is where I use the load summarize, chain I'm going to use the mapreduce, method let me get my chain ready and, then I'm going to say chain dot run and, run it on the docks that we just split, up great now we have our output and I've, actually set it to verbose equals true, to show you what's going on here but, link chain is creating a chain for us, that is going to sequentially call the, different summaries on the different, mini chunks that we want and it's going, to say write a concise summary of the, following and then it gives us the text, and you'll notice that it'll do this, four times to get our four summaries so, there's one there is two there's three, and there's four and then it's going to, ask for a final summary a summary of the, summaries of those four and that's where, it says write a concise summary so we, got our fifth one and then finally we, get our article discusses and it's a you, know a good amount of text for a summary, here the next thing we're going to look, at is how to do question and answering, using documents as context it's an, extremely common example for people to, want to chat their documents meaning, you're going to ask questions to a book, or ask questions to a PDF or a, transcript or whatever it may be and the, way that you're going to do that is, you're going to take the important parts, of your body of text and you're going to, give those to your language model and, then you're going to ask questions for, it and the language model has the, ability to reference the text that you, shared with it and answer your question, with it at the same time, okay so let's look at a simple example, here and again we're going to do this, manually first and then we're going to, build up to it but the context that I'm, sharing is going to be Rachel's 30 years, old Bob is 45 Kevin is 65. and then the, question I'm going to ask is who is, under 40 years old and the convention, that we're going to do is with our, language model we're going to pass our, context plus our question and that's, going to give us our answer, let me run that and then we're going to, do is we're going to combine them so in, this case I'm going to do language model, context plus question equals the output, and then let's see what the output is, here Rachel is under 40 years old and, that is the question I asked and Rachel, yes is the one that's under 40 years old, so this is an extremely easy example of, context plus your question but it gets, complicated pretty quickly here, you run these all right and then the way, that gets more complicated is the next, method is we could use embeddings now, embeddings is going to be a vector, representation of your different chunks, or different documents and that's just a, fancy way of saying your documents are, going to get encoded into a big long, Vector that is much easier to do, similarity comparing on there's a whole, bunch of imports here but basically, you're going to import your vector store, you're going to import the way that, you're going to retrieve your vectors, you're going to call your text loader, which is how you load up your texts into, some documents and then you're going to, call your embedding engine as well, let me let's run that and then we're, going to load up a longer document here, I think because Paul Graham's longest, one so far and it has about 75 000, characters in it now that's pretty long, and we won't be able to put that into, our prompt nor would we really want to, because a secret with working with, language models is you only want to pass, the most relevant context to your, language model you don't want to pass, everything why well because you want to, reduce the signal to noise ratio I like, to offload my language models as much as, I can okay we're going to split up the, docs using our recursive character text, splitter again and then now let's take a, look at how many documents we have so we, have 29 documents with an average of 2, 900 characters so instead of one, document and a big one we have 29, documents that are split into smaller, pieces let's create embeddings for them, and let's go ahead and throw them within, our doc search here, now that we have those embeddings we're, going to create our retrieval engine so, our retrieval engine is going to be the, actual mechanism that goes and looks at, the vector store Compares it with our, docs Compares it with our question and, returns back what we need for it so I'm, going to create a query what does the, author describe as good work which I, know this answer is in the essay there, we'll run that, let's see what it says and we get our, answer back the author describes, painting writing essays and Building, Things That Will Last as good work cool, the next topic we're going to look at is, extraction and extraction is the process, of getting data that you want from a, document some of the common use cases, for this one is to extract user intent, in order to insert a row inside of a, database or extract user intent in order, to make a correct API call query so it's, whenever you want to pull data out of, something and get it in a structured, form so first we're going to import our, packages and I'm going to use a chat, model which is why you see some more, Imports around message types and then, we're also going to import our, structured output parser and our, response schema which is going to be an, easy way for us to get the data that we, want out, so the very first way again vanilla, extraction how we build on top of this, is you will give a sentence with fruit, names and then extract those fruit names, and assign an emoji to them return the, fruit names and Emoji in a python, dictionary so here I'm instructing the, model as to how I wanted to respond to, me, let's go ahead and run this we give it, fruit names apple pear and this is a, kiwi just to give it a kind of more, complicated example and then we're going, to create our prompt which is our, instruction fruit names we're going to, get our output and then all of a sudden, we get a python dictionary of apple, apple pear emojis Etc but I also printed, out the type of this as well and you'll, notice that it's a string because a, language model cannot directly output a, python dictionary however it can output, a string that looks like a python, dictionary and then what we can do if we, did this the manual way is we could just, call eval on top of that string and it's, going to Output a or it's going to read, that dictionary for us and actually, convert it into a dictionary so we get, the same thing but now it's addict okay, however that's not going to work for, very complicated examples and you'll, notice that I had to worry about prompt, engineering I don't want to worry about, prompt engineering so I'm going to use a, lang chain convention and use their, response schema so in this case I'm, going to define the type of response, that I want the language model to return, back to me in and in this case I want to, extract user intent about a song and, artist they want to play so I'm going to, say artist song I'm going to give a, description about it so that the, language model can help me out a little, bit more and then I'm going to create a, structured output parser which is the, thing that is going to read the string, output and give me what I need back and, one of the nice Parts about this, convention is that with this output, parser we can get format instructions, which is what we actually passed to the, language model to tell it what we need, if we ran this you can see that we get a, string back and it says the output, should be a markdown code snippet, formatted in the following schema and, then it gives us the schema that we need, the important part about this is I, didn't have to worry about the prompt, engineering and the Lang chain crew, they're smart crew they're going to make, sure that this is going to be the most, well-engineered prompt that I need to, get my output out so when you can, offload The Prompt engineer in the link, gen and don't worry about it too much, yourself of course unless you need to, customize I'm going to have a chat, prompt template and I'm going to say, given a command from the user extract, the artist and the song names and then, I'm going to give it the format, instructions which is the string up, above and then I'm going to give it the, user prompt whatever the user may say, go ahead and run this and then we format, our prompt and I'm going to say I really, like so young by Portugal the Man, and let's see what this prompt would, actually look like uh so given a command, from the user extract it this is the, format instructions we saw before and my, query gets put on the end of that and, then what we'll do here is I'm going to, pass this prompt to the chat model I'm, going to get the output and let's see, what it looks like we have an artist, Portugal the Man and then a song So, Young which is pretty convenient that we, can go use later down the line and, you'll notice that the output is not a, string this time it's a dictionary that, we can use and that is because the, output parser read the string and, returned us the dictionary which is cool, for more advanced parsing I would, recommend checking out core which is a, library by Eugene yurtzev and it is a, pretty sweet example about how to get, really complicated structured data out, of a piece of text because the output of, language models can vary drastically, evaluation is pretty hard to do however, link chain has some conventions for us, that makes it uh makes it easy for us to, do evaluation on how our models are, actually doing the important part about, this is you really want to be able to, test your pipelines to see if there's, any regressions or if your pipelines are, still doing as well as you want them to, and evaluation is how we're going to be, doing that we're going to do the vector, dance again and the new part that we're, going to import here is on the, evaluation.qa and so we're going to have, an evaluation question and answer chain, it's a link chain will help us out, telling us how our question and answer, is actually done, we're going to load in the long text, that we had before we're going to split, the documents I'm brushing over this, because we already did it we're going to, embed those and do the vector Dance by, the way the vector dance is just what I, call chunking splitting and betting, vectorizing querying etc etc, and then let's make our chain to go get, the information now what I'm going to do, here is I'm going to have a list of, questions and answers this is a python, list with multiple questions so for the, very first one which company sold the, microcomputer kit that his friend built, himself and I went and I validated this, answer myself this is the ground truth, answer the source of Truth at least, there's much of a source of truth that, you can be confident in yourself in it's, going to be health kit and then for the, second question I purposely made it a, hard question that I knew the language, model likely wouldn't get what is the, small City he talked about in a city, that is the financial capital of the USA, so this would take a couple of uh jumps, to figure out what it is the correct, answer is Yorkville there and then what, we're going to do is we're going to call, chain dot apply on this list of question, and answers and you may be asking, yourself well how does link chain know, which what's my question what's my, answer within these well the input key, up here says question and that means, it's going to use this key in order for, the prompt when it passes it to the llm, so it's going to pull the question from, here let me run this and we'll get our, predictions back questions is not, defined great forgot to do that let's, run this and when we get our predictions, back what we have is we have our, original dictionary with our question, and our answer but now we get a result, and the result is going to be the output, of the language model so the, microprocessor computer kit was sold by, healthkit yep that looks good the small, City he talked about is New York City, the financial capital of the USA uh no, it's not it's Yorkville so let's go and, run this through our evaluator and see, if we can get it an incorrect or correct, response so I'm going to uh visualize my, evaluate chain, and then I'm going to execute that chain, and get graded outputs the important, part is that you pass it your question, and answers you pass it the predictions, from up above and then you pass it the, keys that correspond to the ground truth, the question and which answers came from, the language model, let's run this and we get a label that, is correct and incorrect which is the, right output based off of what we saw, before the next major use case that, we're going to look at with Lang chain, is going to be querying tabular data now, this is super important because you're, going to want your language model to, talk to your database or to talk to an, Excel spreadsheet or a financial report, or a financial model or whatever it may, be and you're gonna have to query with, tabular data the way that we're going to, do this is we're going to import from, langchain and we're going to import our, SQL database so it knows how to talk to, the database and then we're also going, to import our SQL database chain so we, can do some discovery about our tables, and about which columns we need to use, we're going to get our connection ready, now I made a quick sqlite database here, about the trees in San Francisco we're, going to get that one going and then we, need to initialize our DB chain and this, is where we're going to use the SQL, database chain here we're going to pass, it our language model and we're going to, pass it our database and I'm going to, put verbose equals true here so you can, see what's going on, around this and so how many species of, tree are there I should say are there in, San Francisco and let's see what it says, here, so it's going to enter into the chain, and it's going to do a select count, distinct Q species from SF trees and, it's going to tell us there's 578. now, this is pretty cool and there's a lot, happening underneath the hood that I, want to call out here so not only did it, find which table to use because there's, multiple datas in the database it found, which column to use and yes species is, in the word but I didn't tell it what, the column name was specifically but it, was smart enough to go ahead and do that, it constructed the right SQL query, executed that query got the result and, returned the natural language response, back to us and you see here that the, response back from the database which is, 578 but it took that 578 and put it in a, natural language response which is super, cool for us and you can see how you can, combine this with your output parsers, where if you didn't want a natural, language response you wanted a, structured response you could you could, go ahead and do that here, now let's go ahead and check this via, pandas just to make sure it worked so, we're going to use pandas instead of our, connection query uh read SQL query, Connection close and let's go ahead and, run this yes 578 that is exactly correct, the next topic we're going to look at is, code understanding so there's a ton of, development happening helping users, query their own code or do code, generation and the way you do that is, you basically load up your documentation, in similar forms that we've seen, beforehand so again these are all pretty, standard Imports and we're going to do, the embedding dance again and what we're, going to do is query the documentation, of the fuzz which is a personal Indie, favorite of mine which helps find string, similarities not using language models, and what we're doing here is this, function we're just going to go through, each folder and load up each file as a, doc and then we're going to put it, inside a list of documents here, let's take a look at just a sample to, make sure we know what we're looking at, we have 175 documents and here's the, start of one of the documents you can, see it's basically just pure code and, this is the documentation that we just, ingested let's create our doc search, let's get our retrieval chain ready and, then I'm going to ask what function do I, do if I wanted to find the most similar, item in a list of items, let's go ahead and run this and see what, it says you could use the process, extract one function from the fuzz, library to find the most similar item in, a list of items nice that looks cool but, what if I just wanted the code well it's, as easy as just altering your question, that you're asking the llm can you write, code to use the process exact one, function only respond with code no other, text or explanation and what we get is, process extract one query Choice One and, choice two that's cool because I can go, ahead and take this and put it right, inside my code the next topic we're, going to look at is when link chain, helps facilitate a language model, talking to an API so the only, non-standard thing we're going to import, here is going to be the API chain and, what I wanted to do is Show an example, about how API documents are read and, then the language model can understand, which API call it needs to make so I, wrote some pretty Scrappy documentation, here that is sloppy on purpose so I put, in a base URL and we're going to go, query restcountries.com and then I put, in two API endpoints here and I also put, in woo this is my documentation just to, show you there is no standard format of, what we need to do here and I'm going to, create the API chain I'm going to pass, in the language model the docs I'm going, to set for both equals true so we can, see what's going on and then my first, question is going to be around this, first endpoint just to show you how I, can switch up here and then so tell me, information about France, it's going to go it's going to make this, request here and it's going to insert, the right token that we need and then, it's going to get the API response back, now this is a bunch of gobbledygook, garbage however it can interpret that, response and give us back a natural, language answer here which is pretty, cool so France is officially assigned, independent country located in Western, Europe et cetera et cetera nice but what, if we asked a different question can you, tell me about the currency cop I wanted, to query this second endpoint here and, you'll notice that I'm still just, calling chain new DOT run I haven't, switched anything else except the query, that I put in here but the language, model is smart enough to understand hey, I need to use a different endpoint is, I'm going to put in cop to this endpoint, here we get the response the currency of, Colombia is the Colombian Peso cop, symbolized by the dollar sign nice the, next topic we're going to look at is, chat Bots and everyone or their mother, is doing chop outs right now because not, only are they easy to build but they're, pretty cool to use and provide to your, users as well the new import we're going, to bring it here is going to be the, conversation buffer memory now the, important thing to know about chat Bots, is it's just the same tools that we've, been using thus far with the addition of, one interesting one which is memory now, memory is when you remember what were, the previous calls that you made to your, language model beforehand so you can, keep context about a conversation and if, you didn't have memory well it would, kind of just be question and answering, with uh that wouldn't be very helpful to, the user in a chat setting so this, conversation buffer memory that's going, to keep a running list of messages that, we've sent back and forth with the AI, I'm saying template you are a chatbot, that is unhelpful your goal is to not, help the user but only make jokes take, what the user is saying and make a joke, out of it we put a placeholder for chat, history which is going to be where our, previous messages are going and then we, also have human input which is going to, be the most recent query that the User, submitted and then we're going to have, the output from the chatbot, let's run this it not defines it great, and then what we see here is I've stored, within our within a memory variable we, have the conversation buffer memory and, the memory key is going to be the chat, history meaning this key is what matches, within your template that you're working, with here then let's create our llm, chain and we're also going to pass in, memory here as well, go ahead and run that and then the first, thing I want to ask is is a pear a fruit, or a vegetable and let's see what it, says here's the template that I put in, beforehand it has is a fruit a pair of, vegetable neither it is an imperative so, there's the joke because I told it to, not be uh unhelp or not be helpful so, then where the memory piece comes into I, say what is one of the fruits that I, first asked you about let's run this, let's see what it says and the cool part, is you can see it has my question that's, in there and it has its response and, then it has my most recent question you, want to know if a pear is a fruit or, vegetable but I'd say it's a non or I'd, say it's a comparative not a very good, joke but I think gpt4 would do a better, job at this, that's chatbots there's a lot of, different types of memories and it can, get complicated pretty quickly here but, I've put a link to the documentation, that you can go and check out yourself, and then the last topic that we're going, to talk about here is Agents now agents, is one of the most exciting places and, one of the most complicated topics, within Lang chain and language models in, general agents are the decision makers, that can look at data understand what, the next action should be and then go, and execute that action with a series of, tools that they've been given access to, the interesting Imports we need to bring, in here is going to be the load tools, and initialize agent now the tools that, we're actually going to load are going, to be the Google search API wrapper and, then the text requests wrapper and this, is going to be a way for us to give our, language model access to the outside, world now if you want to sign up for the, Google search wrapper you need a few API, keys but I put links for those in there, as well let's initialize our language, model and then we're going to put our, tools into a set of variables so here we, have a search and we have a request as, well then we're going to make a toolkit, which is kind of like a tool belt where, you put your hammer and your nails well, now you need a toolkit with your tools, First Tool we're going to use is our, Search tool and we're going to give it a, description this description helps the, language model know when it should use, that tool over another one and then we, have our requests useful for when you, need to make a request to our URL cool, then we need to initialize our agent, we're going to pass in our toolkit our, language model the type of agent that it, should be shows that the type of, decisions or how it should be making, decisions we're going to set verbose, equals true so we can see what's going, on underneath there and return, intermediate steps, so we're going to ask a question to our, agent what is the capital of Canada and, let's see what it does here so it says I, need to find out what the capital of, Canada is yes correct and the action, it's going to do or the tool it's going, to use is your use the search tool that, we had up above and it's going to input, to Google because it's using the Google, one what is the capital of Canada and, it's going to get a response back from, Google which is the observation that it, saw and then it's thought which is, basically it's how it processed that, response Ottawa is the capital of Canada, and we say Ottawa is the capital of, Canada correct now with our agent I'm, going to say tell me what comments, tell me what the comments are about on, this webpage on this webpage and I'm, going to pass it the URL of a hacker, news article and let's see what it says, about that, so the first thing it's going to do is, it's going to say I need to find out, what the comments are about it tries to, use the search tool and it searches for, comments on this URL but no good Google, search was found okay so then it's like, you know I should try to access the web, page directly which is the correct, action it should be doing and it's going, to say it's going to go to the request, tool it's going to put in that URL it's, going to get the HTML back blah blah, blah and it's going to say the comments, around open AIS embedding and API GPT, like LMS sweet our agent had to reason, through the different steps that it, should take and that's the cool part, about agents but it's also working get, kind of complicated and Sam Whitmore on, Twitter actually said in her experience, she's seen that agents are successful, about 85 of each Loop so that means that, each time you're running through an, agent it might not actually work this is, an active area of development and a, space that I'm going to be watching very, closely wow you just made it all the way, down to the bottom and to the end of, this video and I congratulate you where, do we go from here well it's time to, build and as AI matures the use cases, are going to keep growing and growing so, please let me know what else I need to, add to this notebook here now again if, you want to check out examples about, Lang chain projects head over to the, gallery in this repo and like always if, you have additions to this notebook as, well please check out the contribution, guide and throw up a PR thank you crew, we'll see you later"
}